#!/usr/bin/env python3
"""
Step 5: Generate infer output and logits
Completes the pipeline by running the infer step and LM head to produce logits for next token.
"""

import json
import numpy as np
import coremltools as ct
import os
from datetime import datetime
from transformers import AutoTokenizer

def run_lm_head(lm_head_model, hidden_states):
    """Run LM Head model to get token predictions (from typo_fixer_complete.py)."""
    # Ensure we have single token input (1, 1, 1024)
    if hidden_states.shape[1] > 1:
        hidden_states = hidden_states[:, -1:, :]  # Take last token
    
    result = lm_head_model.predict({"hidden_states": hidden_states.astype(np.float16)})
    
    # Combine all logits parts
    all_logits = []
    for i in range(1, 17):  # 16 parts
        key = f"logits{i}"
        if key in result:
            all_logits.append(result[key])
    
    combined_logits = np.concatenate(all_logits, axis=-1)
    return combined_logits

def main():
    """Run infer step and generate logits for next token."""
    # Load prefill output data from step 4
    input_file = "test_generation/step_4_prefill_output.json"
    print(f"Loading data from {input_file}...")
    
    with open(input_file, 'r') as f:
        step4_data = json.load(f)
    
    next_token_pos = step4_data["data"]["next_token_pos"]
    context_pos = step4_data["data"]["context_pos"]
    
    print(f"Next token position: {next_token_pos}")
    print(f"Context position: {context_pos}")
    print()
    
    # Load models
    model_dir = "/Users/mazdahewitt/projects/train-typo-fixer/models/qwen-typo-fixer-ane-flex"
    
    # Load embeddings model
    embeddings_path = os.path.join(model_dir, "qwen-typo-fixer_embeddings.mlpackage")
    embeddings_model = ct.models.MLModel(embeddings_path)
    
    # Load infer model  
    infer_path = os.path.join(model_dir, "qwen-typo-fixer_FFN_chunk_01of01.mlpackage")
    infer_model = ct.models.MLModel(infer_path)
    
    # Load LM head model
    lm_head_path = os.path.join(model_dir, "qwen-typo-fixer_lm_head.mlpackage")
    lm_head_model = ct.models.MLModel(lm_head_path)
    
    # Load tokenizer for token analysis
    tokenizer = AutoTokenizer.from_pretrained("mazhewitt/qwen-typo-fixer", trust_remote_code=True)
    
    print("Models loaded successfully")
    print()
    
    # For testing, we'll simulate generating the next token
    # We need a current token to start the infer process
    # Let's use the last token from our original sequence
    
    # Load the original input_ids from step 1
    step1_file = "test_generation/step_1_tokens.json"
    with open(step1_file, 'r') as f:
        step1_data = json.load(f)
    
    original_input_ids = np.array(step1_data["data"]["input_ids"], dtype=np.int32)
    
    # Get the last actual token (before padding)
    current_token = original_input_ids[:, context_pos-1:context_pos]  # [1, 1]
    print(f"Current token for infer: {current_token} (shape: {current_token.shape})")
    
    # Decode the token to see what it is
    token_text = tokenizer.decode(current_token[0], skip_special_tokens=False)
    print(f"Current token text: '{token_text}'")
    print()
    
    # Run embeddings on current token
    print("Running embeddings on current token...")
    embeddings_result = embeddings_model.predict({"input_ids": current_token})
    hidden_states = embeddings_result['hidden_states']
    print(f"Embeddings output shape: {hidden_states.shape}")
    
    # Prepare infer inputs (following typo_fixer_complete.py)
    position_ids = np.array([next_token_pos-1], dtype=np.int32)
    
    # Create single token causal mask
    context_length = 256
    causal_mask = np.full((1, 1, 1, context_length), -np.inf, dtype=np.float16)
    causal_mask[:, :, 0, :next_token_pos] = 0  # Allow attention to all previous positions
    
    print(f"Infer inputs prepared:")
    print(f"  Hidden states: {hidden_states.shape}")
    print(f"  Position IDs: {position_ids.shape} = {position_ids}")
    print(f"  Causal mask: {causal_mask.shape}")
    print()
    
    # Run infer model
    print("Running infer model...")
    infer_inputs = {
        'hidden_states': hidden_states.astype(np.float16),
        'position_ids': position_ids,
        'causal_mask': causal_mask,
        'current_pos': position_ids
    }
    
    # Note: In practice, we'd need the KV state from prefill, but for testing we'll create a fresh one
    kv_state = infer_model.make_state()
    infer_output = infer_model.predict(infer_inputs, kv_state)
    
    infer_hidden_states = infer_output['output_hidden_states']
    print(f"Infer output hidden states shape: {infer_hidden_states.shape}")
    
    # Run LM head to get logits
    print("Running LM head...")
    logits = run_lm_head(lm_head_model, infer_hidden_states)
    print(f"Final logits shape: {logits.shape}")
    
    # Analyze the logits
    top_5_indices = np.argsort(logits[0, 0])[-5:][::-1]  # Top 5 token predictions
    top_5_logits = logits[0, 0][top_5_indices]
    
    print(f"\nTop 5 predicted tokens:")
    for i, (idx, logit) in enumerate(zip(top_5_indices, top_5_logits)):
        token_text = tokenizer.decode([idx], skip_special_tokens=False)
        print(f"  {i+1}. Token {idx}: '{token_text}' (logit: {logit:.4f})")
    
    # Create output data with all pipeline results
    output_data = {
        "metadata": {
            "step": "5_infer_and_logits",
            "input_file": input_file,
            "model_dir": model_dir,
            "next_token_pos": next_token_pos,
            "context_pos": context_pos,
            "timestamp": datetime.now().isoformat(),
            "description": "Complete pipeline from current token to next token logits"
        },
        "data": {
            "current_token": current_token.tolist(),
            "current_token_text": token_text,
            "embeddings_output": {
                "hidden_states": hidden_states.tolist(),
                "shape": list(hidden_states.shape),
                "dtype": str(hidden_states.dtype)
            },
            "infer_inputs": {
                "position_ids": position_ids.tolist(),
                "causal_mask_shape": list(causal_mask.shape),
                "causal_mask_dtype": str(causal_mask.dtype)
            },
            "infer_output": {
                "hidden_states": infer_hidden_states.tolist(),
                "shape": list(infer_hidden_states.shape),
                "dtype": str(infer_hidden_states.dtype)
            },
            "final_logits": {
                "logits": logits.tolist(),
                "shape": list(logits.shape),
                "dtype": str(logits.dtype)
            },
            "top_predictions": {
                "indices": top_5_indices.tolist(),
                "logits": top_5_logits.tolist(),
                "tokens": [tokenizer.decode([idx], skip_special_tokens=False) for idx in top_5_indices]
            }
        }
    }
    
    # Save to file
    output_file = "test_generation/step_5_infer_and_logits.json"
    with open(output_file, 'w') as f:
        json.dump(output_data, f, indent=2)
    
    print(f"\nâœ… Saved complete pipeline data to {output_file}")
    print(f"   Current token: {current_token[0][0]} ('{token_text}')")
    print(f"   Logits shape: {logits.shape}")
    print(f"   Top prediction: Token {top_5_indices[0]} ('{tokenizer.decode([top_5_indices[0]], skip_special_tokens=False)}')")
    print(f"\nðŸŽ‰ Complete pipeline test data generated!")
    print(f"   Files created: step_1_tokens.json â†’ step_5_infer_and_logits.json")

if __name__ == "__main__":
    main()