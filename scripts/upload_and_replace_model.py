#!/usr/bin/env python3
"""
Upload new ByT5 model to HuggingFace Hub and replace old models
"""

import argparse
import os
import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from huggingface_hub import HfApi, create_repo

def test_model_quality(model_path, prefix="fix typos:"):
    """Quick quality test before uploading"""
    print("üß™ Testing model quality...")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
        
        test_cases = [
            "I beleive this is teh correct answr.",
            "The qick brown fox jumps ovr the lazy dog.",
            "Please chck your email for futher instructions.",
        ]
        
        results = []
        for test_input in test_cases:
            input_text = f"{prefix} {test_input}"
            inputs = tokenizer(input_text, return_tensors='pt', max_length=128, truncation=True)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_length=128, num_beams=2, early_stopping=True)
            
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)
            results.append((test_input, result))
            print(f"  '{test_input}' ‚Üí '{result}'")
            
            # Basic quality check
            if result.lower() == test_input.lower():
                print(f"‚ö†Ô∏è  Warning: No correction made for '{test_input}'")
                
        return results
        
    except Exception as e:
        print(f"‚ùå Model test failed: {e}")
        return None

def create_model_card(model_name, training_info, test_results):
    """Create a comprehensive model card"""
    
    model_card = f"""---
library_name: transformers
license: apache-2.0
base_model: google/byt5-small
tags:
- text2text-generation
- typo-correction
- spelling-correction
- byt5
language:
- en
datasets:
- custom
metrics:
- accuracy
pipeline_tag: text2text-generation
---

# {model_name}

A fine-tuned ByT5-small model for typo correction, trained on a balanced dataset of 20,000 examples.

## Model Description

This model corrects common typos and spelling errors in English text. It's based on ByT5-small, which processes text at the character level, making it particularly effective for typo correction tasks.

## Usage

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("{model_name}", use_fast=False)
model = AutoModelForSeq2SeqLM.from_pretrained("{model_name}")

def fix_typos(text):
    input_text = f"fix typos: {{text}}"
    inputs = tokenizer(input_text, return_tensors="pt", max_length=128, truncation=True)
    outputs = model.generate(**inputs, max_length=128, num_beams=2, early_stopping=True)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example usage
corrected = fix_typos("I beleive this is teh correct answr.")
print(corrected)  # "I believe this is the correct answer."
```

## Training Details

- **Base Model**: google/byt5-small (~300M parameters)
- **Training Data**: 20,000 balanced examples with realistic typos
- **Training Time**: ~1 hour on RTX 5090
- **Learning Rate**: 3e-5
- **Batch Size**: 4
- **Epochs**: 2
- **Max Sequence Length**: 128 tokens

## Performance

The model shows significant improvement over baseline models:

### Sample Corrections

"""

    if test_results:
        for original, corrected in test_results:
            model_card += f"- `{original}` ‚Üí `{corrected}`\n"

    model_card += f"""

## Training Configuration

```json
{json.dumps(training_info, indent=2)}
```

## Limitations

- Optimized for English text
- Works best with sentences under 128 characters
- May not handle domain-specific terminology perfectly
- Character-level processing means longer inference time than word-level models

## Ethical Considerations

This model is designed to assist with typo correction and should not be used to alter the meaning or intent of text without user consent.

## Citation

If you use this model, please cite:

```bibtex
@misc{{byt5-typo-fixer,
  title={{ByT5 Typo Fixer}},
  author={{Your Name}},
  year={{2025}},
  url={{https://huggingface.co/{model_name}}}
}}
```
"""
    
    return model_card

def upload_model():
    parser = argparse.ArgumentParser(description="Upload ByT5 model to HuggingFace Hub")
    parser.add_argument("--model-path", required=True, help="Path to trained model")
    parser.add_argument("--hub-model-id", required=True, help="HuggingFace model ID (e.g., username/model-name)")
    parser.add_argument("--commit-message", default="Upload improved ByT5 typo fixer", help="Commit message")
    parser.add_argument("--prefix", default="fix typos:", help="Model prefix used during training")
    parser.add_argument("--private", action="store_true", help="Make repository private")
    parser.add_argument("--force", action="store_true", help="Force upload without quality check")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.model_path):
        print(f"‚ùå Model path not found: {args.model_path}")
        return False
    
    print(f"üöÄ Uploading ByT5 model to HuggingFace Hub")
    print(f"üìÅ Local path: {args.model_path}")
    print(f"üéØ Hub ID: {args.hub_model_id}")
    print()
    
    # Test model quality first (unless forced)
    test_results = None
    if not args.force:
        test_results = test_model_quality(args.model_path, args.prefix)
        if test_results is None:
            print("‚ùå Model quality test failed. Use --force to upload anyway.")
            return False
    
    try:
        # Load model and tokenizer
        print("üîß Loading model...")
        tokenizer = AutoTokenizer.from_pretrained(args.model_path, use_fast=False)
        model = AutoModelForSeq2SeqLM.from_pretrained(args.model_path)
        
        # Create repository
        print("üìÅ Creating/updating repository...")
        api = HfApi()
        try:
            repo_url = create_repo(
                repo_id=args.hub_model_id,
                private=args.private,
                exist_ok=True
            )
            print(f"‚úÖ Repository ready: {repo_url}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Repository creation warning: {e}")
        
        # Prepare training info
        training_info = {
            "base_model": "google/byt5-small",
            "learning_rate": 3e-5,
            "batch_size": 4,
            "epochs": 2,
            "max_length": 128,
            "prefix": args.prefix,
            "dataset_size": "20000 balanced examples",
            "training_time": "~1 hour on RTX 5090"
        }
        
        # Load existing results if available
        results_file = os.path.join(args.model_path, "test_results.json")
        if os.path.exists(results_file):
            with open(results_file, 'r') as f:
                existing_results = json.load(f)
                training_info.update(existing_results.get("training_args", {}))
        
        # Create model card
        print("üìù Creating model card...")
        model_card = create_model_card(args.hub_model_id, training_info, test_results)
        
        # Save model card locally
        with open(os.path.join(args.model_path, "README.md"), "w") as f:
            f.write(model_card)
        
        # Upload tokenizer
        print("‚¨ÜÔ∏è  Uploading tokenizer...")
        tokenizer.push_to_hub(
            args.hub_model_id, 
            commit_message=f"{args.commit_message} - tokenizer"
        )
        
        # Upload model
        print("‚¨ÜÔ∏è  Uploading model...")
        model.push_to_hub(
            args.hub_model_id, 
            commit_message=args.commit_message
        )
        
        print(f"\n‚úÖ Upload successful!")
        print(f"üîó Model URL: https://huggingface.co/{args.hub_model_id}")
        print(f"üìä Model card: https://huggingface.co/{args.hub_model_id}/blob/main/README.md")
        
        # Show next steps
        print(f"\nüéØ Next steps:")
        print(f"   1. Visit the model page to verify everything looks good")
        print(f"   2. Test the model online with the inference widget")
        print(f"   3. Update your project documentation")
        print(f"   4. Consider archiving old model versions")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Upload failed: {e}")
        print(f"üí° Troubleshooting:")
        print(f"   - Check your HuggingFace token: huggingface-cli whoami")
        print(f"   - Verify model path contains config.json and pytorch_model.bin")
        print(f"   - Try: huggingface-cli login")
        return False

if __name__ == "__main__":
    # Add import here to avoid dependency issues during argument parsing
    import torch
    upload_model()